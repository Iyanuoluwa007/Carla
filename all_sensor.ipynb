{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fbf8dd9-5311-42b0-b95b-11b8550d0cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import carla\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import subprocess\n",
    "import open3d as o3d\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a9d7574-b01a-4de0-8fda-aa604a08236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new client instance\n",
    "client = carla.Client('localhost', 2000)\n",
    "client.set_timeout(10.0)\n",
    "\n",
    "world = client.get_world()\n",
    "\n",
    "bp_lib = world.get_blueprint_library()\n",
    "spawn_points = world.get_map().get_spawn_points()\n",
    "\n",
    "vehicle_bp = bp_lib.find('vehicle.dodge.charger')\n",
    "vehicle = world.try_spawn_actor(vehicle_bp, random.choice(spawn_points))\n",
    "\n",
    "spectator = world.get_spectator()\n",
    "transform = carla.Transform(vehicle.get_transform().transform(carla.Location(x=-4, z=2.5)), vehicle.get_transform().rotation)\n",
    "spectator.set_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32f1c804-9fcc-4321-b834-38b80f007271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the traffic light controller in the background\n",
    "traffic_light_process = subprocess.Popen(['python', 'traffic_light_controller.py'])\n",
    "traffic_process = subprocess.Popen(['python', 'generate_traffic.py', '--number-of-vehicles', '30', '--number-of-walkers', '50'])\n",
    "time.sleep(5)\n",
    "\n",
    "# Set up traffic manager\n",
    "traffic_manager = client.get_trafficmanager()\n",
    "vehicle.set_autopilot(True, traffic_manager.get_port())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8949758e-90f9-4aa6-ae80-a9d736b79646",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIRIDIS = np.array(matplotlib.colormaps.get_cmap('plasma').colors)\n",
    "VID_RANGE = np.linspace(0.0, 1.0, VIRIDIS.shape[0])\n",
    "COOL_RANGE = np.linspace(0.0, 1.0, VIRIDIS.shape[0])\n",
    "COOL = np.array(matplotlib.colormaps.get_cmap('winter')(COOL_RANGE))\n",
    "COOL = COOL[:, :3]  # Remove alpha channel\n",
    "\n",
    "camera_init_trans = carla.Transform(carla.Location(x=0.0, y=0.0, z=1.6), carla.Rotation(pitch=0.0))\n",
    "\n",
    "def set_camera_attributes(camera_bp):\n",
    "    camera_bp.set_attribute('image_size_x', '800')\n",
    "    camera_bp.set_attribute('image_size_y', '600')\n",
    "    camera_bp.set_attribute('fov', '90')\n",
    "\n",
    "# RGB Camera\n",
    "rgb_camera_bp = bp_lib.find('sensor.camera.rgb')\n",
    "set_camera_attributes(rgb_camera_bp)\n",
    "rgb_camera = world.spawn_actor(rgb_camera_bp, camera_init_trans, attach_to=vehicle)\n",
    "\n",
    "# Semantic Segmentation Camera\n",
    "sem_camera_bp = bp_lib.find('sensor.camera.semantic_segmentation')\n",
    "set_camera_attributes(sem_camera_bp)\n",
    "sem_camera = world.spawn_actor(sem_camera_bp, camera_init_trans, attach_to=vehicle)\n",
    "\n",
    "# Instance Segmentation Camera\n",
    "inst_camera_bp = bp_lib.find('sensor.camera.instance_segmentation')\n",
    "set_camera_attributes(inst_camera_bp)\n",
    "inst_camera = world.spawn_actor(inst_camera_bp, camera_init_trans, attach_to=vehicle)\n",
    "\n",
    "# Depth Camera\n",
    "depth_camera_bp = bp_lib.find('sensor.camera.depth')\n",
    "set_camera_attributes(depth_camera_bp)\n",
    "depth_camera = world.spawn_actor(depth_camera_bp, camera_init_trans, attach_to=vehicle)\n",
    "# GNSS Sensor\n",
    "gnss_bp = bp_lib.find('sensor.other.gnss')\n",
    "gnss_sensor = world.spawn_actor(gnss_bp, carla.Transform(), attach_to=vehicle)\n",
    "\n",
    "# IMU Sensor\n",
    "imu_bp = bp_lib.find('sensor.other.imu')\n",
    "imu_sensor = world.spawn_actor(imu_bp, carla.Transform(), attach_to=vehicle)\n",
    "\n",
    "# Collision Sensor\n",
    "collision_bp = bp_lib.find('sensor.other.collision')\n",
    "collision_sensor = world.spawn_actor(collision_bp, carla.Transform(), attach_to=vehicle)\n",
    "\n",
    "# Obstacle Sensor â€“ note the adjusted transform\n",
    "obstacle_bp = bp_lib.find('sensor.other.obstacle')\n",
    "obstacle_bp.set_attribute('hit_radius', '2')\n",
    "obstacle_bp.set_attribute('distance', '50')\n",
    "obstacle_sensor = world.spawn_actor(obstacle_bp, \n",
    "                                    carla.Transform(carla.Location(x=2.5, z=1.5)),\n",
    "                                    attach_to=vehicle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acf838ca-94cb-4eda-9a02-e8c60f4e0b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Initialize image data storage and sensor data dictionary\n",
    "# ---------------------------\n",
    "image_w = 800\n",
    "image_h = 600\n",
    "sensor_data = {\n",
    "    'rgb_image': np.zeros((image_h, image_w, 4), dtype=np.uint8),\n",
    "    'sem_image': np.zeros((image_h, image_w, 4), dtype=np.uint8),\n",
    "    'inst_image': np.zeros((image_h, image_w, 4), dtype=np.uint8),\n",
    "    'depth_image': np.zeros((image_h, image_w, 4), dtype=np.uint8),\n",
    "    # Extra sensor fields:\n",
    "    'gnss': [0, 0],\n",
    "    'imu': {\n",
    "        'gyro': carla.Vector3D(),\n",
    "        'accel': carla.Vector3D(),\n",
    "        'compass': 0\n",
    "    },\n",
    "    'collision': False,\n",
    "    'obstacle': []\n",
    "}\n",
    "\n",
    "# ---------------------------\n",
    "# Define helper functions for overlaying labels and sensor info\n",
    "# ---------------------------\n",
    "def add_label(image, label, position=(10, 50), font_scale=0.8, color=(255, 255, 255)):\n",
    "    labeled_image = image.copy()\n",
    "    cv2.putText(labeled_image, label, position, cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                font_scale, color, 2, cv2.LINE_AA)\n",
    "    return labeled_image\n",
    "\n",
    "# Camera image callbacks\n",
    "def rgb_callback(image, data_dict):\n",
    "    img = np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))\n",
    "    # Save a plain copy (without overlays)\n",
    "    data_dict['rgb_image_raw'] = img.copy()\n",
    "    # Create a copy with label for the tiled view\n",
    "    data_dict['rgb_image'] = add_label(img, \"RGB Camera\")\n",
    "\n",
    "def sem_callback(image, data_dict):\n",
    "    image.convert(carla.ColorConverter.CityScapesPalette)\n",
    "    img = np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))\n",
    "    data_dict['sem_image'] = add_label(img, \"Semantic Segmentation\")\n",
    "\n",
    "def inst_callback(image, data_dict):\n",
    "    img = np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))\n",
    "    data_dict['inst_image'] = add_label(img, \"Instance Segmentation\")\n",
    "\n",
    "def depth_callback(image, data_dict):\n",
    "    image.convert(carla.ColorConverter.LogarithmicDepth)\n",
    "    img = np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))\n",
    "    data_dict['depth_image'] = add_label(img, \"Depth Camera\")\n",
    "\n",
    "# Additional sensor callbacks\n",
    "def gnss_callback(data, data_dict):\n",
    "    data_dict['gnss'] = [data.latitude, data.longitude]\n",
    "\n",
    "def imu_callback(data, data_dict):\n",
    "    data_dict['imu'] = {\n",
    "        'gyro': data.gyroscope,\n",
    "        'accel': data.accelerometer,\n",
    "        'compass': data.compass\n",
    "    }\n",
    "\n",
    "def collision_callback(event, data_dict):\n",
    "    data_dict['collision'] = True\n",
    "\n",
    "# For projecting obstacle positions onto the image we need an intrinsic matrix.\n",
    "def build_projection_matrix(w, h, fov):\n",
    "    focal = w / (2.0 * np.tan(fov * np.pi / 360.0))\n",
    "    K = np.identity(3)\n",
    "    K[0, 0] = K[1, 1] = focal\n",
    "    K[0, 2] = w / 2.0\n",
    "    K[1, 2] = h / 2.0\n",
    "    return K\n",
    "\n",
    "def get_image_point(loc, K, w2c):\n",
    "    # Create a homogeneous point\n",
    "    point = np.array([loc.x, loc.y, loc.z, 1])\n",
    "    point_camera = np.dot(w2c, point)\n",
    "    # Adjust coordinate order (if necessary)\n",
    "    point_camera = [point_camera[1], -point_camera[2], point_camera[0]]\n",
    "    point_img = np.dot(K, point_camera)\n",
    "    point_img[0] /= point_img[2]\n",
    "    point_img[1] /= point_img[2]\n",
    "    return tuple(map(int, point_img[0:2]))\n",
    "\n",
    "def obstacle_callback(event, data_dict, camera, K, image_w, image_h):\n",
    "    # Only consider vehicles or walkers\n",
    "    if not ('vehicle' in event.other_actor.type_id or 'walker' in event.other_actor.type_id):\n",
    "        return\n",
    "\n",
    "    data_dict.setdefault('obstacle', []).append({\n",
    "        'transform': event.other_actor.type_id,\n",
    "        'frame': event.frame\n",
    "    })\n",
    "\n",
    "    world_2_camera = np.array(camera.get_transform().get_inverse_matrix())\n",
    "    obstacle_location = event.other_actor.get_location()\n",
    "    image_point = get_image_point(obstacle_location, K, world_2_camera)\n",
    "    if 'rgb_image' not in data_dict or data_dict['rgb_image'] is None:\n",
    "        return\n",
    "    if 0 < image_point[0] < image_w and 0 < image_point[1] < image_h:\n",
    "        cv2.circle(data_dict['rgb_image'], tuple(image_point), 10, (0, 0, 255), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f53cc9a-3637-4a82-86a1-33b923061b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple compass drawing function\n",
    "def draw_compass(img, theta):\n",
    "    compass_center = (700, 100)\n",
    "    compass_size = 50\n",
    "    cardinal_directions = [\n",
    "        ('N', [0, -1]),\n",
    "        ('E', [1, 0]),\n",
    "        ('S', [0, 1]),\n",
    "        ('W', [-1, 0])\n",
    "    ]\n",
    "    for label, offset in cardinal_directions:\n",
    "        cv2.putText(\n",
    "            img, label,\n",
    "            (int(compass_center[0] + 1.2 * compass_size * offset[0]), \n",
    "             int(compass_center[1] + 1.2 * compass_size * offset[1])),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 100, 200), 2\n",
    "        )\n",
    "    compass_point = (\n",
    "        int(compass_center[0] + compass_size * math.sin(theta)),\n",
    "        int(compass_center[1] - compass_size * math.cos(theta))\n",
    "    )\n",
    "    cv2.line(img, compass_center, compass_point, (0, 140, 255), 3)\n",
    "\n",
    "rgb_camera.listen(lambda image: rgb_callback(image, sensor_data))\n",
    "sem_camera.listen(lambda image: sem_callback(image, sensor_data))\n",
    "inst_camera.listen(lambda image: inst_callback(image, sensor_data))\n",
    "depth_camera.listen(lambda image: depth_callback(image, sensor_data))\n",
    "time.sleep(5)\n",
    "\n",
    "gnss_sensor.listen(lambda data: gnss_callback(data, sensor_data))\n",
    "imu_sensor.listen(lambda data: imu_callback(data, sensor_data))\n",
    "collision_sensor.listen(lambda event: collision_callback(event, sensor_data))\n",
    "obstacle_sensor.listen(lambda event: obstacle_callback(event, sensor_data, rgb_camera,\n",
    "                                                         build_projection_matrix(image_w, image_h, 90),\n",
    "                                                         image_w, image_h))\n",
    "\n",
    "lidar_bp = bp_lib.find('sensor.lidar.ray_cast_semantic')\n",
    "lidar_bp.set_attribute('range', '100.0')\n",
    "lidar_bp.set_attribute('upper_fov', '15.0')\n",
    "lidar_bp.set_attribute('lower_fov', '-25.0')\n",
    "lidar_bp.set_attribute('channels', '64')\n",
    "lidar_bp.set_attribute('rotation_frequency', '20.0')\n",
    "lidar_bp.set_attribute('points_per_second', '500000')\n",
    "lidar_init_trans = carla.Transform(carla.Location(z=2))\n",
    "lidar = world.spawn_actor(lidar_bp, lidar_init_trans, attach_to=vehicle)\n",
    "time.sleep(5)\n",
    "\n",
    "radar_bp = bp_lib.find('sensor.other.radar')\n",
    "radar_bp.set_attribute('horizontal_fov', '30.0')\n",
    "radar_bp.set_attribute('vertical_fov', '30.0')\n",
    "radar_bp.set_attribute('points_per_second', '10000')\n",
    "radar_init_trans = carla.Transform(carla.Location(z=2))\n",
    "radar = world.spawn_actor(radar_bp, radar_init_trans, attach_to=vehicle)\n",
    "\n",
    "lidar_pcd = o3d.geometry.PointCloud()\n",
    "radar_pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "def get_random_color():\n",
    "    color = np.random.rand(3)\n",
    "    while np.linalg.norm(color) < 0.3:\n",
    "        color = np.random.rand(3)\n",
    "    return color\n",
    "\n",
    "def lidar_callback(point_cloud, pcd):\n",
    "    data = np.copy(np.frombuffer(point_cloud.raw_data, dtype=np.dtype('f4')))\n",
    "    if data.shape[0] == 0:\n",
    "        return\n",
    "    data = np.reshape(data, (-1, 6))\n",
    "    points = data[:, :3]\n",
    "    colors = np.array([get_random_color() for _ in range(points.shape[0])])\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "def radar_callback(data, pcd):\n",
    "    radar_data = np.zeros((len(data), 4))\n",
    "    for i, detection in enumerate(data):\n",
    "        x = detection.depth * math.cos(detection.altitude) * math.cos(detection.azimuth)\n",
    "        y = detection.depth * math.cos(detection.altitude) * math.sin(detection.azimuth)\n",
    "        z = detection.depth * math.sin(detection.altitude)\n",
    "        radar_data[i, :] = [x, y, z, detection.velocity]\n",
    "    intensity = np.abs(radar_data[:, -1])\n",
    "    intensity_safe = np.clip(intensity, 1e-6, None)\n",
    "    intensity_col = 1.0 - np.log(intensity_safe) / np.log(np.exp(-0.004 * 100))\n",
    "    int_color = np.c_[\n",
    "        np.interp(intensity_col, COOL_RANGE, COOL[:, 0]),\n",
    "        np.interp(intensity_col, COOL_RANGE, COOL[:, 1]),\n",
    "        np.interp(intensity_col, COOL_RANGE, COOL[:, 2])\n",
    "    ]\n",
    "    points = radar_data[:, :3]\n",
    "    points[:, :1] = -points[:, :1]\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(int_color)\n",
    "\n",
    "lidar.listen(lambda data: lidar_callback(data, lidar_pcd))\n",
    "radar.listen(lambda data: radar_callback(data, radar_pcd))\n",
    "cv2.namedWindow('All Cameras', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('All Cameras', 1280, 960)\n",
    "cv2.namedWindow('Duplicate RGB', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('Duplicate RGB', 800, 600)\n",
    "\n",
    "def preprocess_image(img):\n",
    "    if img.shape[2] == 4:\n",
    "        img = img[:, :, :3]\n",
    "    return img\n",
    "\n",
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window(window_name='Lidar View', width=960, height=540, left=400, top=270)\n",
    "vis.get_render_option().background_color = [0.05, 0.05, 0.05]\n",
    "vis.get_render_option().point_size = 1\n",
    "vis.get_render_option().show_coordinate_frame = True\n",
    "\n",
    "def add_open3d_axis(vis):\n",
    "    axis = o3d.geometry.LineSet()\n",
    "    axis.points = o3d.utility.Vector3dVector(np.array([\n",
    "        [0.0, 0.0, 0.0],\n",
    "        [0.1, 0.0, 0.0],\n",
    "        [0.0, 0.1, 0.0],\n",
    "        [0.0, 0.0, 0.1]\n",
    "    ]))\n",
    "    axis.lines = o3d.utility.Vector2iVector(np.array([\n",
    "        [0, 1],\n",
    "        [0, 2],\n",
    "        [0, 3]\n",
    "    ]))\n",
    "    axis.colors = o3d.utility.Vector3dVector(np.array([\n",
    "        [1, 0, 0],\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 1]\n",
    "    ]))\n",
    "    vis.add_geometry(axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e7a6936-9845-478e-be4e-9a7276f3db63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Open3D WARNING] The number of points is 0 when creating axis-aligned bounding box.\n"
     ]
    }
   ],
   "source": [
    "add_open3d_axis(vis)\n",
    "\n",
    "lidar_added = False\n",
    "radar_added = False\n",
    "\n",
    "# Counter for collision display duration\n",
    "collision_counter = 20\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # First, create a copy of the current raw RGB image (without overlays)\n",
    "        if 'rgb_image_raw' in sensor_data:\n",
    "            duplicate_rgb = sensor_data['rgb_image_raw'].copy()\n",
    "        else:\n",
    "            duplicate_rgb = np.zeros((image_h, image_w, 4), dtype=np.uint8)\n",
    "\n",
    "        rgb_img = sensor_data['rgb_image'].copy()\n",
    "        font = cv2.FONT_HERSHEY_PLAIN # cv2.FONT_HERSHEY_SIMPLEX\n",
    "        fontScale = 0.5\n",
    "        fontScale_2 = 1.\n",
    "        fontColor = (255, 255, 255)\n",
    "        thickness = 2\n",
    "\n",
    "        start_y = 500  # starting y coordinate for the sensor overlays\n",
    "        line_spacing = 20  # space between lines\n",
    "\n",
    "        lat_val = sensor_data['gnss'][0] * 100000\n",
    "        long_val = sensor_data['gnss'][1] * 100000\n",
    "\n",
    "        cv2.putText(rgb_img, \n",
    "                    'Lat: ' + f\"{lat_val:.2f}\",\n",
    "                    (10, start_y), font, fontScale_2, fontColor, thickness, cv2.LINE_AA)\n",
    "        cv2.putText(rgb_img, \n",
    "                    'Long: ' + f\"{long_val:.2f}\",\n",
    "                    (10, start_y + line_spacing), font, fontScale_2, fontColor, thickness, cv2.LINE_AA)\n",
    "\n",
    "        accel = sensor_data['imu']['accel'] - carla.Vector3D(x=0, y=0, z=9.81)\n",
    "        accel_val = accel.length() * 100\n",
    "\n",
    "        cv2.putText(rgb_img, \n",
    "                    'Accel: ' + f\"{accel_val:.2f}\",\n",
    "                    (10, start_y + 2 * line_spacing), font, fontScale_2, fontColor, thickness, cv2.LINE_AA)\n",
    "        \n",
    "        gyro_val = sensor_data['imu']['gyro'].length() * 100\n",
    "        cv2.putText(rgb_img, \n",
    "                    'Gyro: ' + f\"{gyro_val:.2f}\",\n",
    "                    (10, start_y + 3 * line_spacing), font, fontScale_2, fontColor, thickness, cv2.LINE_AA)\n",
    "\n",
    "        \n",
    "        compass_val = sensor_data['imu']['compass'] * 10\n",
    "        cv2.putText(rgb_img, \n",
    "                    'Compass: ' + f\"{compass_val:.2f}\",\n",
    "                    (10, start_y + 4 * line_spacing), font, fontScale_2, fontColor, thickness, cv2.LINE_AA)\n",
    "        \n",
    "        draw_compass(rgb_img, sensor_data['imu']['compass'])\n",
    "\n",
    "        # Display collision alert (if any)\n",
    "        if sensor_data['collision']:\n",
    "            collision_counter -= 1\n",
    "            if collision_counter <= 1:\n",
    "                sensor_data['collision'] = False\n",
    "            cv2.putText(rgb_img, \n",
    "                        'COLLISION',  \n",
    "                        (250, 300), font, 2, (255, 255, 255), 3, cv2.LINE_AA)\n",
    "        else:\n",
    "            collision_counter = 20\n",
    "\n",
    "        # Update the overlay version of the rgb_image\n",
    "        sensor_data['rgb_image'] = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2BGRA)\n",
    "\n",
    "        # Process and prepare images for display (drop alpha channel if needed)\n",
    "        processed_data = {key: preprocess_image(img) for key, img in sensor_data.items() if 'image' in key and key != 'rgb_image_raw'}\n",
    "\n",
    "        # Create a tiled display:\n",
    "        # Top row: RGB (with overlays) and Semantic\n",
    "        top_row = np.concatenate([processed_data['rgb_image'], processed_data['sem_image']], axis=1)\n",
    "        # Bottom row: Depth and Instance\n",
    "        bottom_row = np.concatenate([processed_data['depth_image'], processed_data['inst_image']], axis=1)\n",
    "        tiled = np.concatenate((top_row, bottom_row), axis=0)\n",
    "        cv2.imshow('All Cameras', tiled)\n",
    "        # Show the plain duplicate RGB window (without overlays)\n",
    "        cv2.imshow('Duplicate RGB', preprocess_image(duplicate_rgb))\n",
    "        \n",
    "        # Update Open3D visualizer with LiDAR and Radar point clouds\n",
    "        if not lidar_added:\n",
    "            vis.add_geometry(lidar_pcd)\n",
    "            lidar_added = True\n",
    "        if not radar_added:\n",
    "            vis.add_geometry(radar_pcd)\n",
    "            radar_added = True\n",
    "        vis.update_geometry(lidar_pcd)\n",
    "        vis.update_geometry(radar_pcd)\n",
    "        vis.poll_events()\n",
    "        vis.update_renderer()\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "        time.sleep(0.005)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "vis.destroy_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da13724-bb6b-4a16-bd62-9325d052820c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
