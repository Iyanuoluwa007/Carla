{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fbf8dd9-5311-42b0-b95b-11b8550d0cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import carla\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import subprocess\n",
    "import open3d as o3d\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a9d7574-b01a-4de0-8fda-aa604a08236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new client instance\n",
    "client = carla.Client('localhost', 2000)\n",
    "world = client.get_world()\n",
    "\n",
    "bp_lib = world.get_blueprint_library()\n",
    "spawn_points = world.get_map().get_spawn_points()\n",
    "\n",
    "vehicle_bp = bp_lib.find('vehicle.dodge.charger')\n",
    "vehicle = world.try_spawn_actor(vehicle_bp, random.choice(spawn_points))\n",
    "\n",
    "spectator = world.get_spectator()\n",
    "transform = carla.Transform(vehicle.get_transform().transform(carla.Location(x=-4, z=2.5)), vehicle.get_transform().rotation)\n",
    "spectator.set_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32f1c804-9fcc-4321-b834-38b80f007271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the traffic light controller in the background\n",
    "traffic_light_process = subprocess.Popen(['python', 'traffic_light_controller.py'])\n",
    "traffic_process = subprocess.Popen(['python', 'generate_traffic.py', '--number-of-vehicles', '30', '--number-of-walkers', '50'])\n",
    "time.sleep(5)\n",
    "\n",
    "# Set up traffic manager\n",
    "traffic_manager = client.get_trafficmanager()\n",
    "vehicle.set_autopilot(True, traffic_manager.get_port())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8949758e-90f9-4aa6-ae80-a9d736b79646",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIRIDIS = np.array(matplotlib.colormaps.get_cmap('plasma').colors)\n",
    "VID_RANGE = np.linspace(0.0, 1.0, VIRIDIS.shape[0])\n",
    "COOL_RANGE = np.linspace(0.0, 1.0, VIRIDIS.shape[0])\n",
    "COOL = np.array(matplotlib.colormaps.get_cmap('winter')(COOL_RANGE))\n",
    "COOL = COOL[:, :3]  # Remove alpha channel\n",
    "\n",
    "camera_init_trans = carla.Transform(carla.Location(x=0.0, y=0.0, z=1.6), carla.Rotation(pitch=0.0))\n",
    "\n",
    "def set_camera_attributes(camera_bp):\n",
    "    camera_bp.set_attribute('image_size_x', '800')\n",
    "    camera_bp.set_attribute('image_size_y', '600')\n",
    "    camera_bp.set_attribute('fov', '90')\n",
    "\n",
    "# RGB Camera\n",
    "rgb_camera_bp = bp_lib.find('sensor.camera.rgb')\n",
    "set_camera_attributes(rgb_camera_bp)\n",
    "rgb_camera = world.spawn_actor(rgb_camera_bp, camera_init_trans, attach_to=vehicle)\n",
    "\n",
    "# Semantic Segmentation Camera\n",
    "sem_camera_bp = bp_lib.find('sensor.camera.semantic_segmentation')\n",
    "set_camera_attributes(sem_camera_bp)\n",
    "sem_camera = world.spawn_actor(sem_camera_bp, camera_init_trans, attach_to=vehicle)\n",
    "\n",
    "# Instance Segmentation Camera\n",
    "inst_camera_bp = bp_lib.find('sensor.camera.instance_segmentation')\n",
    "set_camera_attributes(inst_camera_bp)\n",
    "inst_camera = world.spawn_actor(inst_camera_bp, camera_init_trans, attach_to=vehicle)\n",
    "\n",
    "# Depth Camera\n",
    "depth_camera_bp = bp_lib.find('sensor.camera.depth')\n",
    "set_camera_attributes(depth_camera_bp)\n",
    "depth_camera = world.spawn_actor(depth_camera_bp, camera_init_trans, attach_to=vehicle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acf838ca-94cb-4eda-9a02-e8c60f4e0b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare image data storage (define sensor_data here so itâ€™s available everywhere)\n",
    "image_w = 800\n",
    "image_h = 600\n",
    "sensor_data = {\n",
    "    'rgb_image': np.zeros((image_h, image_w, 4), dtype=np.uint8),\n",
    "    'sem_image': np.zeros((image_h, image_w, 4), dtype=np.uint8),\n",
    "    'inst_image': np.zeros((image_h, image_w, 4), dtype=np.uint8),\n",
    "    'depth_image': np.zeros((image_h, image_w, 4), dtype=np.uint8),\n",
    "}\n",
    "\n",
    "def add_label(image, label, position=(10, 50), font_scale=0.8, color=(255, 255, 255)):\n",
    "    \"\"\"Adds a text label to an image.\"\"\"\n",
    "    labeled_image = image.copy()\n",
    "    cv2.putText(labeled_image, label, position, cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                font_scale, color, 2, cv2.LINE_AA)\n",
    "    return labeled_image\n",
    "\n",
    "def rgb_callback(image, data_dict):\n",
    "    img = np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))\n",
    "    data_dict['rgb_image'] = add_label(img, \"RGB Camera\")\n",
    "\n",
    "def sem_callback(image, data_dict):\n",
    "    image.convert(carla.ColorConverter.CityScapesPalette)\n",
    "    img = np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))\n",
    "    data_dict['sem_image'] = add_label(img, \"Semantic Segmentation\")\n",
    "\n",
    "def inst_callback(image, data_dict):\n",
    "    img = np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))\n",
    "    data_dict['inst_image'] = add_label(img, \"Instance Segmentation\")\n",
    "\n",
    "def depth_callback(image, data_dict):\n",
    "    image.convert(carla.ColorConverter.LogarithmicDepth)\n",
    "    img = np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))\n",
    "    data_dict['depth_image'] = add_label(img, \"Depth Camera\")\n",
    "\n",
    "# Start camera listeners\n",
    "rgb_camera.listen(lambda image: rgb_callback(image, sensor_data))\n",
    "sem_camera.listen(lambda image: sem_callback(image, sensor_data))\n",
    "inst_camera.listen(lambda image: inst_callback(image, sensor_data))\n",
    "depth_camera.listen(lambda image: depth_callback(image, sensor_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f53cc9a-3637-4a82-86a1-33b923061b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_bp = bp_lib.find('sensor.lidar.ray_cast_semantic')\n",
    "lidar_bp.set_attribute('range', '100.0')\n",
    "lidar_bp.set_attribute('upper_fov', '15.0')\n",
    "lidar_bp.set_attribute('lower_fov', '-25.0')\n",
    "lidar_bp.set_attribute('channels', '64')\n",
    "lidar_bp.set_attribute('rotation_frequency', '20.0')\n",
    "lidar_bp.set_attribute('points_per_second', '500000')\n",
    "lidar_init_trans = carla.Transform(carla.Location(z=2))\n",
    "lidar = world.spawn_actor(lidar_bp, lidar_init_trans, attach_to=vehicle)\n",
    "\n",
    "# Radar Sensor\n",
    "radar_bp = bp_lib.find('sensor.other.radar')\n",
    "radar_bp.set_attribute('horizontal_fov', '30.0')\n",
    "radar_bp.set_attribute('vertical_fov', '30.0')\n",
    "radar_bp.set_attribute('points_per_second', '10000')\n",
    "radar_init_trans = carla.Transform(carla.Location(z=2))\n",
    "radar = world.spawn_actor(radar_bp, radar_init_trans, attach_to=vehicle)\n",
    "\n",
    "# Initialize Open3D point clouds for LiDAR and Radar\n",
    "lidar_pcd = o3d.geometry.PointCloud()\n",
    "radar_pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "def get_random_color():\n",
    "    color = np.random.rand(3)\n",
    "    while np.linalg.norm(color) < 0.3:\n",
    "        color = np.random.rand(3)\n",
    "    return color\n",
    "\n",
    "def lidar_callback(point_cloud, pcd):\n",
    "    data = np.copy(np.frombuffer(point_cloud.raw_data, dtype=np.dtype('f4')))\n",
    "    if data.shape[0] == 0:\n",
    "        return\n",
    "    data = np.reshape(data, (-1, 6))  # 6 values per point: x, y, z, intensity, object_id, timestamp\n",
    "    points = data[:, :3]\n",
    "    colors = np.array([get_random_color() for _ in range(points.shape[0])])\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "def radar_callback(data, pcd):\n",
    "    radar_data = np.zeros((len(data), 4))\n",
    "    for i, detection in enumerate(data):\n",
    "        x = detection.depth * math.cos(detection.altitude) * math.cos(detection.azimuth)\n",
    "        y = detection.depth * math.cos(detection.altitude) * math.sin(detection.azimuth)\n",
    "        z = detection.depth * math.sin(detection.altitude)\n",
    "        radar_data[i, :] = [x, y, z, detection.velocity]\n",
    "    intensity = np.abs(radar_data[:, -1])\n",
    "    intensity_safe = np.clip(intensity, 1e-6, None)\n",
    "    intensity_col = 1.0 - np.log(intensity_safe) / np.log(np.exp(-0.004 * 100))\n",
    "    int_color = np.c_[\n",
    "        np.interp(intensity_col, COOL_RANGE, COOL[:, 0]),\n",
    "        np.interp(intensity_col, COOL_RANGE, COOL[:, 1]),\n",
    "        np.interp(intensity_col, COOL_RANGE, COOL[:, 2])\n",
    "    ]\n",
    "    points = radar_data[:, :3]\n",
    "    points[:, :1] = -points[:, :1]  # flip x-axis if needed\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(int_color)\n",
    "\n",
    "# Start LiDAR and Radar listeners\n",
    "lidar.listen(lambda data: lidar_callback(data, lidar_pcd))\n",
    "radar.listen(lambda data: radar_callback(data, radar_pcd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e7a6936-9845-478e-be4e-9a7276f3db63",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.namedWindow('All Cameras', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('All Cameras', 1280, 960)\n",
    "cv2.namedWindow('Duplicate RGB', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('Duplicate RGB', 800, 600)\n",
    "\n",
    "def preprocess_image(img):\n",
    "    \"\"\"Ensure the image has 3 channels (drop alpha if present).\"\"\"\n",
    "    if img.shape[2] == 4:\n",
    "        img = img[:, :, :3]\n",
    "    return img\n",
    "\n",
    "\n",
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window(window_name='Carla Lidar', width=960, height=540, left=400, top=270)\n",
    "vis.get_render_option().background_color = [0.05, 0.05, 0.05]\n",
    "vis.get_render_option().point_size = 1\n",
    "vis.get_render_option().show_coordinate_frame = True\n",
    "\n",
    "def add_open3d_axis(vis):\n",
    "    axis = o3d.geometry.LineSet()\n",
    "    axis.points = o3d.utility.Vector3dVector(np.array([\n",
    "        [0.0, 0.0, 0.0],\n",
    "        [0.1, 0.0, 0.0],\n",
    "        [0.0, 0.1, 0.0],\n",
    "        [0.0, 0.0, 0.1]\n",
    "    ]))\n",
    "    axis.lines = o3d.utility.Vector2iVector(np.array([\n",
    "        [0, 1],\n",
    "        [0, 2],\n",
    "        [0, 3]\n",
    "    ]))\n",
    "    axis.colors = o3d.utility.Vector3dVector(np.array([\n",
    "        [1, 0, 0],\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 1]\n",
    "    ]))\n",
    "    vis.add_geometry(axis)\n",
    "\n",
    "add_open3d_axis(vis)\n",
    "\n",
    "lidar_added = False\n",
    "radar_added = False\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # --- Process and display camera images ---\n",
    "        processed_data = {key: preprocess_image(img) for key, img in sensor_data.items()}\n",
    "        # Check if the images have been updated (nonzero height)\n",
    "        if all(img.shape[0] > 0 for img in processed_data.values()):\n",
    "            rgb_clean = processed_data['rgb_image'].copy()\n",
    "            top_row = np.concatenate([processed_data['rgb_image'], processed_data['sem_image']], axis=1)\n",
    "            bottom_row = np.concatenate([processed_data['depth_image'], processed_data['inst_image']], axis=1)\n",
    "            tiled = np.concatenate((top_row, bottom_row), axis=0)\n",
    "            cv2.imshow('All Cameras', tiled)\n",
    "            cv2.imshow('Duplicate RGB', rgb_clean)\n",
    "        \n",
    "        # --- Update Open3D visualizer with LiDAR and Radar data ---\n",
    "        if not lidar_added:\n",
    "            vis.add_geometry(lidar_pcd)\n",
    "            lidar_added = True\n",
    "        if not radar_added:\n",
    "            vis.add_geometry(radar_pcd)\n",
    "            radar_added = True\n",
    "        vis.update_geometry(lidar_pcd)\n",
    "        vis.update_geometry(radar_pcd)\n",
    "        vis.poll_events()\n",
    "        vis.update_renderer()\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "        \n",
    "        time.sleep(0.005)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "vis.destroy_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e197426-a050-45f6-907d-dfdd732a7e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stop and destroy sensors\n",
    "# lidar.stop()\n",
    "# lidar.destroy()\n",
    "# radar.stop()\n",
    "# radar.destroy()\n",
    "# rgb_camera.stop()\n",
    "# rgb_camera.destroy()\n",
    "# sem_camera.stop()\n",
    "# sem_camera.destroy()\n",
    "# inst_camera.stop()\n",
    "# inst_camera.destroy()\n",
    "# depth_camera.stop()\n",
    "# depth_camera.destroy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
