{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34030af6-192e-4964-811a-5632e793117e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "# Imports and Initializations\n",
    "import carla\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import subprocess\n",
    "import open3d as o3d\n",
    "import matplotlib\n",
    "\n",
    "# Create a new client instance\n",
    "client = carla.Client('localhost', 2000)\n",
    "client.set_timeout(10.0)\n",
    "\n",
    "world = client.get_world()\n",
    "bp_lib = world.get_blueprint_library()\n",
    "spawn_points = world.get_map().get_spawn_points()\n",
    "\n",
    "vehicle_bp = bp_lib.find('vehicle.dodge.charger')\n",
    "vehicle = world.try_spawn_actor(vehicle_bp, random.choice(spawn_points))\n",
    "\n",
    "spectator = world.get_spectator()\n",
    "transform = carla.Transform(vehicle.get_transform().transform(carla.Location(x=-4, z=2.5)), vehicle.get_transform().rotation)\n",
    "spectator.set_transform(transform)\n",
    "\n",
    "# Start traffic-related processes in the background\n",
    "traffic_light_process = subprocess.Popen(['python', 'traffic_light_controller.py'])\n",
    "traffic_process = subprocess.Popen(['python', 'generate_traffic.py', '--number-of-vehicles', '20', '--number-of-walkers', '30'])\n",
    "time.sleep(5)\n",
    "\n",
    "# Set up traffic manager\n",
    "traffic_manager = client.get_trafficmanager()\n",
    "vehicle.set_autopilot(True, traffic_manager.get_port())\n",
    "\n",
    "# Color maps for overlays\n",
    "VIRIDIS = np.array(matplotlib.colormaps.get_cmap('plasma').colors)\n",
    "VID_RANGE = np.linspace(0.0, 1.0, VIRIDIS.shape[0])\n",
    "COOL_RANGE = np.linspace(0.0, 1.0, VIRIDIS.shape[0])\n",
    "COOL_RANGE = np.linspace(0.0, 1.0, VIRIDIS.shape[0])\n",
    "COOL = np.array(matplotlib.colormaps.get_cmap('winter')(COOL_RANGE))\n",
    "COOL = COOL[:, :3]  # Remove alpha channel\n",
    "\n",
    "camera_init_trans = carla.Transform(carla.Location(x=0.0, y=0.0, z=1.6), carla.Rotation(pitch=0.0))\n",
    "\n",
    "def set_camera_attributes(camera_bp):\n",
    "    camera_bp.set_attribute('image_size_x', '800')\n",
    "    camera_bp.set_attribute('image_size_y', '600')\n",
    "    camera_bp.set_attribute('fov', '90')\n",
    "\n",
    "# RGB Camera\n",
    "rgb_camera_bp = bp_lib.find('sensor.camera.rgb')\n",
    "set_camera_attributes(rgb_camera_bp)\n",
    "rgb_camera = world.spawn_actor(rgb_camera_bp, camera_init_trans, attach_to=vehicle)\n",
    "\n",
    "# Semantic Segmentation Camera\n",
    "sem_camera_bp = bp_lib.find('sensor.camera.semantic_segmentation')\n",
    "set_camera_attributes(sem_camera_bp)\n",
    "sem_camera = world.spawn_actor(sem_camera_bp, camera_init_trans, attach_to=vehicle)\n",
    "\n",
    "# Instance Segmentation Camera\n",
    "inst_camera_bp = bp_lib.find('sensor.camera.instance_segmentation')\n",
    "set_camera_attributes(inst_camera_bp)\n",
    "inst_camera = world.spawn_actor(inst_camera_bp, camera_init_trans, attach_to=vehicle)\n",
    "\n",
    "# Depth Camera\n",
    "depth_camera_bp = bp_lib.find('sensor.camera.depth')\n",
    "set_camera_attributes(depth_camera_bp)\n",
    "depth_camera = world.spawn_actor(depth_camera_bp, camera_init_trans, attach_to=vehicle)\n",
    "\n",
    "# GNSS Sensor\n",
    "gnss_bp = bp_lib.find('sensor.other.gnss')\n",
    "gnss_sensor = world.spawn_actor(gnss_bp, carla.Transform(), attach_to=vehicle)\n",
    "\n",
    "# IMU Sensor\n",
    "imu_bp = bp_lib.find('sensor.other.imu')\n",
    "imu_sensor = world.spawn_actor(imu_bp, carla.Transform(), attach_to=vehicle)\n",
    "\n",
    "# Collision Sensor\n",
    "collision_bp = bp_lib.find('sensor.other.collision')\n",
    "collision_sensor = world.spawn_actor(collision_bp, carla.Transform(), attach_to=vehicle)\n",
    "\n",
    "# Obstacle Sensor â€“ note the adjusted transform\n",
    "obstacle_bp = bp_lib.find('sensor.other.obstacle')\n",
    "obstacle_bp.set_attribute('hit_radius', '2')\n",
    "obstacle_bp.set_attribute('distance', '50')\n",
    "obstacle_sensor = world.spawn_actor(obstacle_bp, \n",
    "                                    carla.Transform(carla.Location(x=2.5, z=1.5)),\n",
    "                                    attach_to=vehicle)\n",
    "\n",
    "# ---------------------------\n",
    "# Initialize image data storage and sensor data dictionary\n",
    "# ---------------------------\n",
    "image_w = 800\n",
    "image_h = 600\n",
    "sensor_data = {\n",
    "    'rgb_image': np.zeros((image_h, image_w, 4), dtype=np.uint8),\n",
    "    'sem_image': np.zeros((image_h, image_w, 4), dtype=np.uint8),\n",
    "    'inst_image': np.zeros((image_h, image_w, 4), dtype=np.uint8),\n",
    "    'depth_image': np.zeros((image_h, image_w, 4), dtype=np.uint8),\n",
    "    # Extra sensor fields:\n",
    "    'gnss': [0, 0],\n",
    "    'imu': {\n",
    "        'gyro': carla.Vector3D(),\n",
    "        'accel': carla.Vector3D(),\n",
    "        'compass': 0\n",
    "    },\n",
    "    'collision': False,\n",
    "    'obstacle': []\n",
    "}\n",
    "\n",
    "# ---------------------------\n",
    "# Define helper functions for overlays and sensor info\n",
    "# ---------------------------\n",
    "def add_label(image, label, position=(10, 50), font_scale=0.8, color=(255, 255, 255)):\n",
    "    labeled_image = image.copy()\n",
    "    cv2.putText(labeled_image, label, position, cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                font_scale, color, 2, cv2.LINE_AA)\n",
    "    return labeled_image\n",
    "\n",
    "# Camera image callbacks\n",
    "def rgb_callback(image, data_dict):\n",
    "    img = np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))\n",
    "    data_dict['rgb_image_raw'] = img.copy()  # Plain copy without overlays\n",
    "    data_dict['rgb_image'] = add_label(img, \"RGB Camera\")\n",
    "\n",
    "def sem_callback(image, data_dict):\n",
    "    image.convert(carla.ColorConverter.CityScapesPalette)\n",
    "    img = np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))\n",
    "    data_dict['sem_image'] = add_label(img, \"Semantic Segmentation\")\n",
    "\n",
    "def inst_callback(image, data_dict):\n",
    "    img = np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))\n",
    "    data_dict['inst_image'] = add_label(img, \"Instance Segmentation\")\n",
    "\n",
    "def depth_callback(image, data_dict):\n",
    "    image.convert(carla.ColorConverter.LogarithmicDepth)\n",
    "    img = np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))\n",
    "    data_dict['depth_image'] = add_label(img, \"Depth Camera\")\n",
    "\n",
    "def gnss_callback(data, data_dict):\n",
    "    data_dict['gnss'] = [data.latitude, data.longitude]\n",
    "\n",
    "def imu_callback(data, data_dict):\n",
    "    data_dict['imu'] = {\n",
    "        'gyro': data.gyroscope,\n",
    "        'accel': data.accelerometer,\n",
    "        'compass': data.compass\n",
    "    }\n",
    "\n",
    "def collision_callback(event, data_dict):\n",
    "    data_dict['collision'] = True\n",
    "\n",
    "# For projecting obstacle positions onto the image we need an intrinsic matrix.\n",
    "def build_projection_matrix(w, h, fov):\n",
    "    focal = w / (2.0 * np.tan(fov * np.pi / 360.0))\n",
    "    K = np.identity(3)\n",
    "    K[0, 0] = K[1, 1] = focal\n",
    "    K[0, 2] = w / 2.0\n",
    "    K[1, 2] = h / 2.0\n",
    "    return K\n",
    "\n",
    "def get_image_point(loc, K, w2c):\n",
    "    point = np.array([loc.x, loc.y, loc.z, 1])\n",
    "    point_camera = np.dot(w2c, point)\n",
    "    point_camera = [point_camera[1], -point_camera[2], point_camera[0]]\n",
    "    point_img = np.dot(K, point_camera)\n",
    "    point_img[0] /= point_img[2]\n",
    "    point_img[1] /= point_img[2]\n",
    "    return tuple(map(int, point_img[0:2]))\n",
    "\n",
    "def obstacle_callback(event, data_dict, camera, K, image_w, image_h):\n",
    "    if not ('vehicle' in event.other_actor.type_id or 'walker' in event.other_actor.type_id):\n",
    "        return\n",
    "    data_dict.setdefault('obstacle', []).append({\n",
    "        'transform': event.other_actor.type_id,\n",
    "        'frame': event.frame\n",
    "    })\n",
    "    world_2_camera = np.array(camera.get_transform().get_inverse_matrix())\n",
    "    obstacle_location = event.other_actor.get_location()\n",
    "    image_point = get_image_point(obstacle_location, K, world_2_camera)\n",
    "    if 'rgb_image' not in data_dict or data_dict['rgb_image'] is None:\n",
    "        return\n",
    "    if 0 < image_point[0] < image_w and 0 < image_point[1] < image_h:\n",
    "        cv2.circle(data_dict['rgb_image'], tuple(image_point), 10, (0, 0, 255), 2)\n",
    "\n",
    "def draw_compass(img, theta):\n",
    "    compass_center = (700, 100)\n",
    "    compass_size = 50\n",
    "    cardinal_directions = [\n",
    "        ('N', [0, -1]),\n",
    "        ('E', [1, 0]),\n",
    "        ('S', [0, 1]),\n",
    "        ('W', [-1, 0])\n",
    "    ]\n",
    "    for label, offset in cardinal_directions:\n",
    "        cv2.putText(\n",
    "            img, label,\n",
    "            (int(compass_center[0] + 1.2 * compass_size * offset[0]), \n",
    "             int(compass_center[1] + 1.2 * compass_size * offset[1])),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 100, 200), 2\n",
    "        )\n",
    "    compass_point = (\n",
    "        int(compass_center[0] + compass_size * math.sin(theta)),\n",
    "        int(compass_center[1] - compass_size * math.cos(theta))\n",
    "    )\n",
    "    cv2.line(img, compass_center, compass_point, (0, 140, 255), 3)\n",
    "\n",
    "# Register sensor callbacks\n",
    "rgb_camera.listen(lambda image: rgb_callback(image, sensor_data))\n",
    "sem_camera.listen(lambda image: sem_callback(image, sensor_data))\n",
    "inst_camera.listen(lambda image: inst_callback(image, sensor_data))\n",
    "depth_camera.listen(lambda image: depth_callback(image, sensor_data))\n",
    "time.sleep(5)\n",
    "gnss_sensor.listen(lambda data: gnss_callback(data, sensor_data))\n",
    "imu_sensor.listen(lambda data: imu_callback(data, sensor_data))\n",
    "collision_sensor.listen(lambda event: collision_callback(event, sensor_data))\n",
    "obstacle_sensor.listen(lambda event: obstacle_callback(event, sensor_data, rgb_camera,\n",
    "                                                         build_projection_matrix(image_w, image_h, 90),\n",
    "                                                         image_w, image_h))\n",
    "\n",
    "# LiDAR & Radar setup (not modified here)\n",
    "lidar_bp = bp_lib.find('sensor.lidar.ray_cast_semantic')\n",
    "lidar_bp.set_attribute('range', '100.0')\n",
    "lidar_bp.set_attribute('upper_fov', '15.0')\n",
    "lidar_bp.set_attribute('lower_fov', '-25.0')\n",
    "lidar_bp.set_attribute('channels', '64')\n",
    "lidar_bp.set_attribute('rotation_frequency', '20.0')\n",
    "lidar_bp.set_attribute('points_per_second', '500000')\n",
    "lidar_init_trans = carla.Transform(carla.Location(z=2))\n",
    "lidar = world.spawn_actor(lidar_bp, lidar_init_trans, attach_to=vehicle)\n",
    "time.sleep(5)\n",
    "\n",
    "radar_bp = bp_lib.find('sensor.other.radar')\n",
    "radar_bp.set_attribute('horizontal_fov', '30.0')\n",
    "radar_bp.set_attribute('vertical_fov', '30.0')\n",
    "radar_bp.set_attribute('points_per_second', '10000')\n",
    "radar_init_trans = carla.Transform(carla.Location(z=2))\n",
    "radar = world.spawn_actor(radar_bp, radar_init_trans, attach_to=vehicle)\n",
    "\n",
    "lidar_pcd = o3d.geometry.PointCloud()\n",
    "radar_pcd = o3d.geometry.PointCloud()\n",
    "\n",
    "def get_random_color():\n",
    "    color = np.random.rand(3)\n",
    "    while np.linalg.norm(color) < 0.3:\n",
    "        color = np.random.rand(3)\n",
    "    return color\n",
    "\n",
    "def lidar_callback(point_cloud, pcd):\n",
    "    data = np.copy(np.frombuffer(point_cloud.raw_data, dtype=np.dtype('f4')))\n",
    "    if data.shape[0] == 0:\n",
    "        return\n",
    "    data = np.reshape(data, (-1, 6))\n",
    "    points = data[:, :3]\n",
    "    colors = np.array([get_random_color() for _ in range(points.shape[0])])\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "def radar_callback(data, pcd):\n",
    "    radar_data = np.zeros((len(data), 4))\n",
    "    for i, detection in enumerate(data):\n",
    "        x = detection.depth * math.cos(detection.altitude) * math.cos(detection.azimuth)\n",
    "        y = detection.depth * math.cos(detection.altitude) * math.sin(detection.azimuth)\n",
    "        z = detection.depth * math.sin(detection.altitude)\n",
    "        radar_data[i, :] = [x, y, z, detection.velocity]\n",
    "    intensity = np.abs(radar_data[:, -1])\n",
    "    intensity_safe = np.clip(intensity, 1e-6, None)\n",
    "    intensity_col = 1.0 - np.log(intensity_safe) / np.log(np.exp(-0.004 * 100))\n",
    "    int_color = np.c_[\n",
    "        np.interp(intensity_col, COOL_RANGE, COOL[:, 0]),\n",
    "        np.interp(intensity_col, COOL_RANGE, COOL[:, 1]),\n",
    "        np.interp(intensity_col, COOL_RANGE, COOL[:, 2])\n",
    "    ]\n",
    "    points = radar_data[:, :3]\n",
    "    points[:, :1] = -points[:, :1]\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(int_color)\n",
    "\n",
    "lidar.listen(lambda data: lidar_callback(data, lidar_pcd))\n",
    "radar.listen(lambda data: radar_callback(data, radar_pcd))\n",
    "\n",
    "cv2.namedWindow('All Cameras', cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('All Cameras', 1280, 960)\n",
    "# We are no longer calling cv2.imshow for the duplicate RGB below\n",
    "# cv2.namedWindow('Duplicate RGB', cv2.WINDOW_NORMAL)\n",
    "# cv2.resizeWindow('Duplicate RGB', 800, 600)\n",
    "\n",
    "def preprocess_image(img):\n",
    "    if img.shape[2] == 4:\n",
    "        img = img[:, :, :3]\n",
    "    return img\n",
    "\n",
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window(window_name='Lidar View', width=960, height=540, left=400, top=270)\n",
    "vis.get_render_option().background_color = [0.05, 0.05, 0.05]\n",
    "vis.get_render_option().point_size = 1\n",
    "vis.get_render_option().show_coordinate_frame = True\n",
    "\n",
    "def add_open3d_axis(vis):\n",
    "    axis = o3d.geometry.LineSet()\n",
    "    axis.points = o3d.utility.Vector3dVector(np.array([\n",
    "        [0.0, 0.0, 0.0],\n",
    "        [0.1, 0.0, 0.0],\n",
    "        [0.0, 0.1, 0.0],\n",
    "        [0.0, 0.0, 0.1]\n",
    "    ]))\n",
    "    axis.lines = o3d.utility.Vector2iVector(np.array([\n",
    "        [0, 1],\n",
    "        [0, 2],\n",
    "        [0, 3]\n",
    "    ]))\n",
    "    axis.colors = o3d.utility.Vector3dVector(np.array([\n",
    "        [1, 0, 0],\n",
    "        [0, 1, 0],\n",
    "        [0, 0, 1]\n",
    "    ]))\n",
    "    vis.add_geometry(axis)\n",
    "add_open3d_axis(vis)\n",
    "\n",
    "lidar_added = False\n",
    "radar_added = False\n",
    "\n",
    "# Counter for collision display duration\n",
    "collision_counter = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a25072e7-c274-4cf1-94a3-ece4f81cc8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\okeiy\\AppData\\Local\\Temp\\ipykernel_4124\\4015216359.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(model_path, map_location=torch.device('cuda'))\n"
     ]
    }
   ],
   "source": [
    "# ByteTrack & YOLO Initialization\n",
    "import os\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Append ByteTrack path (adjust the path as needed)\n",
    "sys.path.append(\"C:/Users/okeiy/OneDrive - University of Salford/Documents/Sch Notes/Dissertation/Codes/Dataset/ByteTrack-ONNX-Sample-main\")\n",
    "from byte_tracker.tracker.byte_tracker import BYTETracker\n",
    "\n",
    "# --- Load YOLO Model ---\n",
    "model_path = r\"C:\\Users\\okeiy\\OneDrive - University of Salford\\Documents\\Sch Notes\\Dissertation\\Codes\\Dataset\\Yolo\\bdd100k\\my_yolov11_model.pt\"\n",
    "model = torch.load(model_path, map_location=torch.device('cuda'))\n",
    "model.model.eval()  # set model to evaluation mode\n",
    "\n",
    "# --- Initialize ByteTrack Tracker ---\n",
    "class TrackerArgs:\n",
    "    def __init__(self):\n",
    "        self.track_thresh = 0.5   # detection score threshold for tracking\n",
    "        self.track_buffer = 30    # frames to keep lost tracks\n",
    "        self.match_thresh = 0.8   # matching threshold\n",
    "        self.mot20 = False\n",
    "args = TrackerArgs()\n",
    "tracker_byte = BYTETracker(args, frame_rate=30)\n",
    "\n",
    "# --- Set parameters for ByteTrack processing ---\n",
    "target_size = 640  # network input size\n",
    "conf_thresh = 0.4  # detection confidence threshold\n",
    "\n",
    "# --- Helper: Letterbox Resizing ---\n",
    "def letterbox(image, new_shape=(640, 640), color=(114, 114, 114)):\n",
    "    shape = image.shape[:2]  # current shape [height, width]\n",
    "    ratio = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    new_unpad = (int(round(shape[1] * ratio)), int(round(shape[0] * ratio)))  # (width, height)\n",
    "    dw = new_shape[1] - new_unpad[0]\n",
    "    dh = new_shape[0] - new_unpad[1]\n",
    "    dw /= 2\n",
    "    dh /= 2\n",
    "    resized = cv2.resize(image, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top = int(round(dh - 0.1))\n",
    "    bottom = int(round(dh + 0.1))\n",
    "    left = int(round(dw - 0.1))\n",
    "    right = int(round(dw + 0.1))\n",
    "    padded = cv2.copyMakeBorder(resized, top, bottom, left, right,\n",
    "                                cv2.BORDER_CONSTANT, value=color)\n",
    "    return padded, ratio, left, top\n",
    "\n",
    "# --- Helper: Compute IoU between two boxes ---\n",
    "def compute_iou(box1, box2):\n",
    "    x_left   = max(box1[0], box2[0])\n",
    "    y_top    = max(box1[1], box2[1])\n",
    "    x_right  = min(box1[2], box2[2])\n",
    "    y_bottom = min(box1[3], box2[3])\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "    inter_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    iou = inter_area / float(box1_area + box2_area - inter_area + 1e-6)\n",
    "    return iou\n",
    "\n",
    "# --- Define the class names ---\n",
    "class_names = [\"bike\", \"bus\", \"car\", \"motor\", \"person\", \"rider\", \"traffic light\", \"traffic sign\", \"train\", \"truck\"]\n",
    "\n",
    "# --- Initialize ByteTrack output video writer ---\n",
    "byte_output_dir = r\"C:\\Users\\okeiy\\Desktop\\New folder\"\n",
    "os.makedirs(byte_output_dir, exist_ok=True)\n",
    "byte_output_video_path = os.path.join(byte_output_dir, \"Carla_Output_3.mp4\")\n",
    "# Assume the original duplicate RGB frame has dimensions image_w x image_h\n",
    "byte_frame_width = image_w\n",
    "byte_frame_height = image_h\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "byte_out = cv2.VideoWriter(byte_output_video_path, fourcc, 30, (byte_frame_width, byte_frame_height))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d5daa69-a473-49ba-bf60-edebcb465c95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 2 cars, 1 truck, 87.3ms\n",
      "Speed: 2.2ms preprocess, 87.3ms inference, 200.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 2 cars, 1 truck, 64.1ms\n",
      "Speed: 19.7ms preprocess, 64.1ms inference, 35.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 2 cars, 1 truck, 116.8ms\n",
      "Speed: 0.0ms preprocess, 116.8ms inference, 13.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 1 car, 1 truck, 122.1ms\n",
      "Speed: 0.0ms preprocess, 122.1ms inference, 17.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 1 car, 1 truck, 101.6ms\n",
      "Speed: 0.0ms preprocess, 101.6ms inference, 19.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 1 car, 1 truck, 92.2ms\n",
      "Speed: 2.2ms preprocess, 92.2ms inference, 21.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 2 cars, 1 truck, 72.5ms\n",
      "Speed: 1.0ms preprocess, 72.5ms inference, 15.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 1 truck, 79.9ms\n",
      "Speed: 1.0ms preprocess, 79.9ms inference, 15.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 1 car, 1 truck, 119.6ms\n",
      "Speed: 1.4ms preprocess, 119.6ms inference, 22.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 1 car, 1 truck, 115.3ms\n",
      "Speed: 0.0ms preprocess, 115.3ms inference, 11.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 1 car, 1 person, 1 traffic light, 1 truck, 105.6ms\n",
      "Speed: 1.0ms preprocess, 105.6ms inference, 38.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 2 cars, 1 traffic light, 1 truck, 94.5ms\n",
      "Speed: 1.0ms preprocess, 94.5ms inference, 34.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 2 cars, 1 truck, 88.5ms\n",
      "Speed: 2.0ms preprocess, 88.5ms inference, 11.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 2 cars, 1 truck, 121.2ms\n",
      "Speed: 1.0ms preprocess, 121.2ms inference, 47.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 3 cars, 1 truck, 81.4ms\n",
      "Speed: 1.0ms preprocess, 81.4ms inference, 17.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 1 car, 1 truck, 90.3ms\n",
      "Speed: 1.0ms preprocess, 90.3ms inference, 15.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 2 cars, 1 person, 107.1ms\n",
      "Speed: 1.0ms preprocess, 107.1ms inference, 10.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 1 car, 106.8ms\n",
      "Speed: 1.5ms preprocess, 106.8ms inference, 39.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 1 car, 92.5ms\n",
      "Speed: 1.0ms preprocess, 92.5ms inference, 18.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 1 car, 150.0ms\n",
      "Speed: 2.0ms preprocess, 150.0ms inference, 12.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 2 cars, 106.1ms\n",
      "Speed: 1.3ms preprocess, 106.1ms inference, 15.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 2 cars, 39.5ms\n",
      "Speed: 1.5ms preprocess, 39.5ms inference, 40.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 1 car, 109.3ms\n",
      "Speed: 2.5ms preprocess, 109.3ms inference, 15.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 2 cars, 1 person, 1 traffic light, 64.0ms\n",
      "Speed: 2.0ms preprocess, 64.0ms inference, 25.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 3 cars, 1 person, 1 traffic light, 128.5ms\n",
      "Speed: 1.0ms preprocess, 128.5ms inference, 21.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 2 cars, 1 person, 1 traffic light, 87.5ms\n",
      "Speed: 0.0ms preprocess, 87.5ms inference, 32.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 2 cars, 1 person, 1 traffic light, 96.5ms\n",
      "Speed: 1.1ms preprocess, 96.5ms inference, 44.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 2 cars, 1 traffic light, 109.2ms\n",
      "Speed: 2.5ms preprocess, 109.2ms inference, 45.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 2 cars, 99.3ms\n",
      "Speed: 1.2ms preprocess, 99.3ms inference, 16.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 2 cars, 104.2ms\n",
      "Speed: 2.0ms preprocess, 104.2ms inference, 34.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 3 cars, 1 person, 104.2ms\n",
      "Speed: 2.3ms preprocess, 104.2ms inference, 36.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 3 cars, 1 person, 114.5ms\n",
      "Speed: 1.5ms preprocess, 114.5ms inference, 34.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 3 cars, 1 person, 58.1ms\n",
      "Speed: 3.0ms preprocess, 58.1ms inference, 8.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 4 cars, 1 person, 31.6ms\n",
      "Speed: 1.2ms preprocess, 31.6ms inference, 7.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 4 cars, 1 person, 27.2ms\n",
      "Speed: 1.0ms preprocess, 27.2ms inference, 10.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 4 cars, 1 person, 74.8ms\n",
      "Speed: 4.4ms preprocess, 74.8ms inference, 9.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 4 cars, 1 person, 31.4ms\n",
      "Speed: 1.2ms preprocess, 31.4ms inference, 5.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 4 cars, 1 person, 28.8ms\n",
      "Speed: 1.0ms preprocess, 28.8ms inference, 9.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 4 cars, 1 person, 15.5ms\n",
      "Speed: 1.3ms preprocess, 15.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 2 cars, 2 persons, 107.8ms\n",
      "Speed: 2.0ms preprocess, 107.8ms inference, 48.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 1 car, 1 person, 122.3ms\n",
      "Speed: 1.2ms preprocess, 122.3ms inference, 11.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 2 cars, 2 persons, 87.0ms\n",
      "Speed: 1.0ms preprocess, 87.0ms inference, 5.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 2 cars, 1 person, 40.0ms\n",
      "Speed: 2.5ms preprocess, 40.0ms inference, 3.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 2 cars, 1 person, 66.8ms\n",
      "Speed: 3.2ms preprocess, 66.8ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 2 cars, 1 person, 87.6ms\n",
      "Speed: 1.0ms preprocess, 87.6ms inference, 15.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 2 cars, 2 persons, 14.7ms\n",
      "Speed: 1.0ms preprocess, 14.7ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 2 cars, 3 persons, 1 truck, 40.4ms\n",
      "Speed: 4.3ms preprocess, 40.4ms inference, 6.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 4 cars, 1 person, 1 truck, 17.0ms\n",
      "Speed: 1.0ms preprocess, 17.0ms inference, 21.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 4 cars, 2 persons, 71.5ms\n",
      "Speed: 1.5ms preprocess, 71.5ms inference, 20.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 3 cars, 2 persons, 121.8ms\n",
      "Speed: 3.5ms preprocess, 121.8ms inference, 5.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 5 cars, 1 person, 64.0ms\n",
      "Speed: 1.0ms preprocess, 64.0ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 4 cars, 3 persons, 66.9ms\n",
      "Speed: 1.0ms preprocess, 66.9ms inference, 5.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 3 cars, 1 person, 71.9ms\n",
      "Speed: 1.2ms preprocess, 71.9ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 3 cars, 1 person, 74.1ms\n",
      "Speed: 2.0ms preprocess, 74.1ms inference, 6.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 2 cars, 1 person, 65.0ms\n",
      "Speed: 1.1ms preprocess, 65.0ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 3 cars, 1 person, 69.8ms\n",
      "Speed: 2.2ms preprocess, 69.8ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 3 cars, 1 person, 77.7ms\n",
      "Speed: 1.0ms preprocess, 77.7ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 3 cars, 2 persons, 59.3ms\n",
      "Speed: 1.0ms preprocess, 59.3ms inference, 3.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 3 cars, 50.8ms\n",
      "Speed: 1.0ms preprocess, 50.8ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 3 cars, 91.6ms\n",
      "Speed: 1.1ms preprocess, 91.6ms inference, 5.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 3 cars, 14.8ms\n",
      "Speed: 0.0ms preprocess, 14.8ms inference, 5.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 3 cars, 1 person, 64.7ms\n",
      "Speed: 2.5ms preprocess, 64.7ms inference, 17.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 4 cars, 63.2ms\n",
      "Speed: 3.1ms preprocess, 63.2ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 3 cars, 1 traffic light, 44.5ms\n",
      "Speed: 1.5ms preprocess, 44.5ms inference, 6.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 3 cars, 69.3ms\n",
      "Speed: 1.3ms preprocess, 69.3ms inference, 5.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 3 cars, 1 person, 1 traffic sign, 93.5ms\n",
      "Speed: 10.5ms preprocess, 93.5ms inference, 5.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 4 cars, 2 traffic signs, 43.9ms\n",
      "Speed: 1.0ms preprocess, 43.9ms inference, 25.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 5 cars, 1 traffic sign, 21.5ms\n",
      "Speed: 1.3ms preprocess, 21.5ms inference, 34.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 3 cars, 1 traffic sign, 121.3ms\n",
      "Speed: 1.1ms preprocess, 121.3ms inference, 8.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 3 cars, 38.1ms\n",
      "Speed: 1.0ms preprocess, 38.1ms inference, 5.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 3 cars, 61.7ms\n",
      "Speed: 0.0ms preprocess, 61.7ms inference, 31.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 2 cars, 1 truck, 51.7ms\n",
      "Speed: 2.0ms preprocess, 51.7ms inference, 18.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 2 cars, 1 truck, 44.4ms\n",
      "Speed: 2.0ms preprocess, 44.4ms inference, 18.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 5 cars, 1 traffic sign, 1 truck, 53.5ms\n",
      "Speed: 2.1ms preprocess, 53.5ms inference, 6.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 4 cars, 1 traffic light, 1 truck, 65.3ms\n",
      "Speed: 1.2ms preprocess, 65.3ms inference, 6.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 4 cars, 55.0ms\n",
      "Speed: 1.3ms preprocess, 55.0ms inference, 6.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 2 cars, 44.7ms\n",
      "Speed: 0.0ms preprocess, 44.7ms inference, 4.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 3 cars, 73.3ms\n",
      "Speed: 3.4ms preprocess, 73.3ms inference, 5.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 2 cars, 63.5ms\n",
      "Speed: 1.1ms preprocess, 63.5ms inference, 12.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 3 cars, 2 traffic lights, 52.8ms\n",
      "Speed: 1.0ms preprocess, 52.8ms inference, 10.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 4 cars, 55.7ms\n",
      "Speed: 0.0ms preprocess, 55.7ms inference, 29.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 2 cars, 1 truck, 59.9ms\n",
      "Speed: 1.0ms preprocess, 59.9ms inference, 15.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 cars, 1 truck, 83.2ms\n",
      "Speed: 1.1ms preprocess, 83.2ms inference, 11.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 cars, 173.8ms\n",
      "Speed: 30.3ms preprocess, 173.8ms inference, 46.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 cars, 80.9ms\n",
      "Speed: 5.8ms preprocess, 80.9ms inference, 7.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 cars, 1 traffic light, 36.5ms\n",
      "Speed: 1.5ms preprocess, 36.5ms inference, 4.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 cars, 51.4ms\n",
      "Speed: 1.2ms preprocess, 51.4ms inference, 4.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 cars, 60.2ms\n",
      "Speed: 1.0ms preprocess, 60.2ms inference, 6.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 cars, 1 traffic light, 59.3ms\n",
      "Speed: 0.0ms preprocess, 59.3ms inference, 4.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 cars, 66.6ms\n",
      "Speed: 0.0ms preprocess, 66.6ms inference, 6.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 3 cars, 1 truck, 66.3ms\n",
      "Speed: 1.0ms preprocess, 66.3ms inference, 4.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 cars, 125.5ms\n",
      "Speed: 3.7ms preprocess, 125.5ms inference, 14.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 cars, 122.2ms\n",
      "Speed: 1.0ms preprocess, 122.2ms inference, 24.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 cars, 92.9ms\n",
      "Speed: 2.0ms preprocess, 92.9ms inference, 15.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 cars, 1 traffic sign, 68.2ms\n",
      "Speed: 1.0ms preprocess, 68.2ms inference, 48.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 3 cars, 54.1ms\n",
      "Speed: 1.7ms preprocess, 54.1ms inference, 16.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 cars, 83.4ms\n",
      "Speed: 3.1ms preprocess, 83.4ms inference, 16.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 cars, 49.1ms\n",
      "Speed: 1.0ms preprocess, 49.1ms inference, 12.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 2 cars, 102.1ms\n",
      "Speed: 2.9ms preprocess, 102.1ms inference, 10.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 cars, 101.1ms\n",
      "Speed: 1.5ms preprocess, 101.1ms inference, 7.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 cars, 56.5ms\n",
      "Speed: 1.1ms preprocess, 56.5ms inference, 12.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 cars, 24.4ms\n",
      "Speed: 1.0ms preprocess, 24.4ms inference, 10.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 cars, 35.8ms\n",
      "Speed: 2.2ms preprocess, 35.8ms inference, 6.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 cars, 111.5ms\n",
      "Speed: 1.0ms preprocess, 111.5ms inference, 12.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 cars, 72.2ms\n",
      "Speed: 1.0ms preprocess, 72.2ms inference, 14.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 cars, 76.4ms\n",
      "Speed: 1.0ms preprocess, 76.4ms inference, 21.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 bus, 1 car, 65.2ms\n",
      "Speed: 1.0ms preprocess, 65.2ms inference, 5.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 cars, 50.3ms\n",
      "Speed: 1.1ms preprocess, 50.3ms inference, 5.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 cars, 60.8ms\n",
      "Speed: 1.0ms preprocess, 60.8ms inference, 5.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 cars, 90.8ms\n",
      "Speed: 1.0ms preprocess, 90.8ms inference, 21.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 cars, 66.1ms\n",
      "Speed: 1.0ms preprocess, 66.1ms inference, 18.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 cars, 116.3ms\n",
      "Speed: 1.0ms preprocess, 116.3ms inference, 13.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 cars, 1 traffic sign, 113.5ms\n",
      "Speed: 1.1ms preprocess, 113.5ms inference, 11.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 cars, 67.3ms\n",
      "Speed: 2.0ms preprocess, 67.3ms inference, 26.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 cars, 61.7ms\n",
      "Speed: 1.0ms preprocess, 61.7ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 cars, 80.1ms\n",
      "Speed: 4.1ms preprocess, 80.1ms inference, 4.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 cars, 16.5ms\n",
      "Speed: 0.0ms preprocess, 16.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 cars, 82.3ms\n",
      "Speed: 1.1ms preprocess, 82.3ms inference, 11.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 6 cars, 117.2ms\n",
      "Speed: 1.5ms preprocess, 117.2ms inference, 12.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 cars, 1 person, 96.5ms\n",
      "Speed: 2.0ms preprocess, 96.5ms inference, 23.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 cars, 55.7ms\n",
      "Speed: 2.3ms preprocess, 55.7ms inference, 3.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 cars, 1 person, 72.1ms\n",
      "Speed: 2.1ms preprocess, 72.1ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 cars, 2 persons, 62.3ms\n",
      "Speed: 1.0ms preprocess, 62.3ms inference, 5.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 cars, 2 persons, 93.0ms\n",
      "Speed: 1.0ms preprocess, 93.0ms inference, 4.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 cars, 2 persons, 68.1ms\n",
      "Speed: 0.0ms preprocess, 68.1ms inference, 5.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 cars, 1 person, 2 traffic signs, 51.2ms\n",
      "Speed: 1.1ms preprocess, 51.2ms inference, 9.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 cars, 2 persons, 69.4ms\n",
      "Speed: 1.0ms preprocess, 69.4ms inference, 5.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 cars, 59.4ms\n",
      "Speed: 1.0ms preprocess, 59.4ms inference, 5.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 cars, 1 person, 1 traffic sign, 55.4ms\n",
      "Speed: 1.1ms preprocess, 55.4ms inference, 4.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 cars, 1 person, 2 traffic signs, 52.0ms\n",
      "Speed: 2.0ms preprocess, 52.0ms inference, 5.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 cars, 1 person, 1 traffic sign, 51.1ms\n",
      "Speed: 1.1ms preprocess, 51.1ms inference, 4.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 cars, 1 person, 1 traffic sign, 68.5ms\n",
      "Speed: 1.5ms preprocess, 68.5ms inference, 14.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 cars, 3 traffic signs, 93.2ms\n",
      "Speed: 4.0ms preprocess, 93.2ms inference, 33.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 cars, 2 traffic signs, 79.8ms\n",
      "Speed: 0.0ms preprocess, 79.8ms inference, 17.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 cars, 1 motor, 2 persons, 1 traffic sign, 75.0ms\n",
      "Speed: 4.6ms preprocess, 75.0ms inference, 9.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 cars, 1 person, 1 traffic sign, 109.0ms\n",
      "Speed: 2.6ms preprocess, 109.0ms inference, 26.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 cars, 1 motor, 1 person, 3 traffic signs, 112.4ms\n",
      "Speed: 2.6ms preprocess, 112.4ms inference, 21.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 cars, 1 person, 2 traffic signs, 81.4ms\n",
      "Speed: 1.4ms preprocess, 81.4ms inference, 14.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 cars, 2 traffic signs, 37.4ms\n",
      "Speed: 1.3ms preprocess, 37.4ms inference, 14.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 cars, 1 motor, 2 persons, 1 traffic light, 1 traffic sign, 40.5ms\n",
      "Speed: 3.5ms preprocess, 40.5ms inference, 8.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 cars, 2 persons, 1 traffic sign, 50.3ms\n",
      "Speed: 2.4ms preprocess, 50.3ms inference, 12.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 cars, 2 persons, 2 traffic signs, 39.5ms\n",
      "Speed: 2.2ms preprocess, 39.5ms inference, 10.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 cars, 1 motor, 2 traffic lights, 2 traffic signs, 118.0ms\n",
      "Speed: 1.1ms preprocess, 118.0ms inference, 11.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 cars, 1 motor, 76.0ms\n",
      "Speed: 1.1ms preprocess, 76.0ms inference, 13.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 cars, 62.6ms\n",
      "Speed: 2.4ms preprocess, 62.6ms inference, 40.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 5 cars, 1 traffic light, 1 traffic sign, 49.5ms\n",
      "Speed: 0.0ms preprocess, 49.5ms inference, 6.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 cars, 44.9ms\n",
      "Speed: 1.3ms preprocess, 44.9ms inference, 5.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 cars, 1 motor, 74.0ms\n",
      "Speed: 1.5ms preprocess, 74.0ms inference, 22.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 cars, 1 traffic light, 182.3ms\n",
      "Speed: 1.1ms preprocess, 182.3ms inference, 20.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 5 cars, 1 traffic sign, 57.0ms\n",
      "Speed: 3.0ms preprocess, 57.0ms inference, 12.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 cars, 1 motor, 1 traffic sign, 68.4ms\n",
      "Speed: 1.0ms preprocess, 68.4ms inference, 10.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 cars, 1 person, 1 traffic light, 1 traffic sign, 70.4ms\n",
      "Speed: 1.0ms preprocess, 70.4ms inference, 15.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 3 cars, 1 motor, 1 person, 79.6ms\n",
      "Speed: 1.5ms preprocess, 79.6ms inference, 13.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 cars, 1 person, 1 traffic sign, 102.9ms\n",
      "Speed: 3.1ms preprocess, 102.9ms inference, 20.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 cars, 1 motor, 2 persons, 1 traffic light, 70.6ms\n",
      "Speed: 0.0ms preprocess, 70.6ms inference, 16.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 cars, 1 motor, 39.9ms\n",
      "Speed: 2.2ms preprocess, 39.9ms inference, 17.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 6 cars, 2 persons, 1 rider, 2 traffic lights, 1 traffic sign, 100.6ms\n",
      "Speed: 2.1ms preprocess, 100.6ms inference, 20.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 4 cars, 1 person, 121.5ms\n",
      "Speed: 1.0ms preprocess, 121.5ms inference, 24.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 5 cars, 2 persons, 1 traffic light, 80.5ms\n",
      "Speed: 1.0ms preprocess, 80.5ms inference, 18.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 5 cars, 2 persons, 1 traffic light, 102.3ms\n",
      "Speed: 1.0ms preprocess, 102.3ms inference, 14.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 5 cars, 2 persons, 1 traffic light, 2 traffic signs, 134.8ms\n",
      "Speed: 1.1ms preprocess, 134.8ms inference, 20.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 73\u001b[0m\n\u001b[0;32m     70\u001b[0m input_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(img_normalized)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 73\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m res \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     75\u001b[0m boxes \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39mboxes\u001b[38;5;241m.\u001b[39mxyxy\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()      \n",
      "File \u001b[1;32m~\\.conda\\envs\\carla-sim\\lib\\site-packages\\ultralytics\\engine\\model.py:181\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    154\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    155\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    157\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m    158\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(source, stream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\carla-sim\\lib\\site-packages\\ultralytics\\engine\\model.py:559\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    558\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 559\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\carla-sim\\lib\\site-packages\\ultralytics\\engine\\predictor.py:175\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\carla-sim\\lib\\site-packages\\torch\\utils\\_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\carla-sim\\lib\\site-packages\\ultralytics\\engine\\predictor.py:261\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 261\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(im, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\carla-sim\\lib\\site-packages\\ultralytics\\engine\\predictor.py:145\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[1;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m    140\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    141\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    144\u001b[0m )\n\u001b[1;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize, embed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\carla-sim\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\carla-sim\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.conda\\envs\\carla-sim\\lib\\site-packages\\ultralytics\\nn\\autobackend.py:555\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:\n\u001b[1;32m--> 555\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:\n",
      "File \u001b[1;32m~\\.conda\\envs\\carla-sim\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\carla-sim\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.conda\\envs\\carla-sim\\lib\\site-packages\\ultralytics\\nn\\tasks.py:109\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\carla-sim\\lib\\site-packages\\ultralytics\\nn\\tasks.py:127\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\carla-sim\\lib\\site-packages\\ultralytics\\nn\\tasks.py:148\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 148\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    149\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32m~\\.conda\\envs\\carla-sim\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\carla-sim\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.conda\\envs\\carla-sim\\lib\\site-packages\\ultralytics\\nn\\modules\\block.py:239\u001b[0m, in \u001b[0;36mC2f.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 239\u001b[0m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\.conda\\envs\\carla-sim\\lib\\site-packages\\ultralytics\\nn\\modules\\block.py:239\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 239\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32m~\\.conda\\envs\\carla-sim\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\.conda\\envs\\carla-sim\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\.conda\\envs\\carla-sim\\lib\\site-packages\\ultralytics\\nn\\modules\\block.py:264\u001b[0m, in \u001b[0;36mC3.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    263\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through the CSP bottleneck with 2 convolutions.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv3(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main Processing Loop with ByteTrack Integration\n",
    "frame_count = 0\n",
    "overall_start_time = time.time()\n",
    "prev_time = time.time()\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # Get the current raw RGB image (without overlays)\n",
    "        if 'rgb_image_raw' in sensor_data:\n",
    "            duplicate_rgb = sensor_data['rgb_image_raw'].copy()\n",
    "        else:\n",
    "            duplicate_rgb = np.zeros((image_h, image_w, 4), dtype=np.uint8)\n",
    "\n",
    "        # Process the overlay image (for tiled display)\n",
    "        rgb_img = sensor_data['rgb_image'].copy()\n",
    "        font = cv2.FONT_HERSHEY_PLAIN\n",
    "        fontScale = 0.5\n",
    "        fontScale_2 = 1.0\n",
    "        fontColor = (255, 255, 255)\n",
    "        thickness = 2\n",
    "\n",
    "        start_y = 500  # starting y coordinate for sensor overlays\n",
    "        line_spacing = 20\n",
    "\n",
    "        lat_val = sensor_data['gnss'][0] * 100000\n",
    "        long_val = sensor_data['gnss'][1] * 100000\n",
    "        cv2.putText(rgb_img, 'Lat: ' + f\"{lat_val:.2f}\", (10, start_y), font, fontScale_2, fontColor, thickness, cv2.LINE_AA)\n",
    "        cv2.putText(rgb_img, 'Long: ' + f\"{long_val:.2f}\", (10, start_y + line_spacing), font, fontScale_2, fontColor, thickness, cv2.LINE_AA)\n",
    "\n",
    "        accel = sensor_data['imu']['accel'] - carla.Vector3D(x=0, y=0, z=9.81)\n",
    "        accel_val = accel.length() * 100\n",
    "        cv2.putText(rgb_img, 'Accel: ' + f\"{accel_val:.2f}\", (10, start_y + 2 * line_spacing), font, fontScale_2, fontColor, thickness, cv2.LINE_AA)\n",
    "        \n",
    "        gyro_val = sensor_data['imu']['gyro'].length() * 100\n",
    "        cv2.putText(rgb_img, 'Gyro: ' + f\"{gyro_val:.2f}\", (10, start_y + 3 * line_spacing), font, fontScale_2, fontColor, thickness, cv2.LINE_AA)\n",
    "\n",
    "        compass_val = sensor_data['imu']['compass'] * 10\n",
    "        cv2.putText(rgb_img, 'Compass: ' + f\"{compass_val:.2f}\", (10, start_y + 4 * line_spacing), font, fontScale_2, fontColor, thickness, cv2.LINE_AA)\n",
    "        draw_compass(rgb_img, sensor_data['imu']['compass'])\n",
    "\n",
    "        # Display collision alert if needed\n",
    "        if sensor_data['collision']:\n",
    "            collision_counter -= 1\n",
    "            if collision_counter <= 1:\n",
    "                sensor_data['collision'] = False\n",
    "            cv2.putText(rgb_img, 'COLLISION', (250, 300), font, 2, (255, 255, 255), 3, cv2.LINE_AA)\n",
    "        else:\n",
    "            collision_counter = 20\n",
    "\n",
    "        sensor_data['rgb_image'] = cv2.cvtColor(rgb_img, cv2.COLOR_BGR2BGRA)\n",
    "\n",
    "        # Prepare images for tiled display (drop alpha channel if needed)\n",
    "        processed_data = {key: preprocess_image(img) for key, img in sensor_data.items() if 'image' in key and key != 'rgb_image_raw'}\n",
    "        top_row = np.concatenate([processed_data['rgb_image'], processed_data['sem_image']], axis=1)\n",
    "        bottom_row = np.concatenate([processed_data['depth_image'], processed_data['inst_image']], axis=1)\n",
    "        tiled = np.concatenate((top_row, bottom_row), axis=0)\n",
    "        cv2.imshow('All Cameras', tiled)\n",
    "\n",
    "        # ---------------------------\n",
    "        # Instead of showing the Duplicate RGB window, feed it to ByteTrack\n",
    "        # ---------------------------\n",
    "        # Preprocess the duplicate RGB (remove alpha channel) and ensure contiguous memory layout\n",
    "        byte_frame = preprocess_image(duplicate_rgb)\n",
    "        byte_frame = np.ascontiguousarray(byte_frame)\n",
    "\n",
    "        # --- ByteTrack Processing ---\n",
    "        padded_frame, ratio, pad_w, pad_h = letterbox(byte_frame, (target_size, target_size))\n",
    "        img_rgb = cv2.cvtColor(padded_frame, cv2.COLOR_BGR2RGB)\n",
    "        img_normalized = img_rgb.astype(np.float32) / 255.0\n",
    "        input_tensor = torch.from_numpy(img_normalized).permute(2, 0, 1).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            results = model(input_tensor)\n",
    "        res = results[0]\n",
    "        boxes = res.boxes.xyxy.cpu().numpy()      \n",
    "        scores = res.boxes.conf.cpu().numpy()       \n",
    "        raw_labels = res.boxes.cls.cpu().numpy()    \n",
    "\n",
    "        valid_inds = scores >= conf_thresh\n",
    "        boxes = boxes[valid_inds]\n",
    "        scores = scores[valid_inds]\n",
    "        det_labels = raw_labels[valid_inds]\n",
    "        detections = np.concatenate([boxes, scores.reshape(-1, 1)], axis=1)\n",
    "\n",
    "        detection_boxes_orig = []\n",
    "        for box in boxes:\n",
    "            x1_orig = (box[0] - pad_w) / ratio\n",
    "            y1_orig = (box[1] - pad_h) / ratio\n",
    "            x2_orig = (box[2] - pad_w) / ratio\n",
    "            y2_orig = (box[3] - pad_h) / ratio\n",
    "            detection_boxes_orig.append([x1_orig, y1_orig, x2_orig, y2_orig])\n",
    "        detection_boxes_orig = np.array(detection_boxes_orig)\n",
    "\n",
    "        img_info = (target_size, target_size)\n",
    "        img_size = (target_size, target_size)\n",
    "        tracks = tracker_byte.update(detections, img_info, img_size)\n",
    "\n",
    "        for track in tracks:\n",
    "            x1, y1, x2, y2 = track.tlbr\n",
    "            x1_orig = int((x1 - pad_w) / ratio)\n",
    "            y1_orig = int((y1 - pad_h) / ratio)\n",
    "            x2_orig = int((x2 - pad_w) / ratio)\n",
    "            y2_orig = int((y2 - pad_h) / ratio)\n",
    "            track_id = track.track_id\n",
    "\n",
    "            track_box_orig = [x1_orig, y1_orig, x2_orig, y2_orig]\n",
    "            best_iou = 0\n",
    "            best_label = None\n",
    "            for i, det_box in enumerate(detection_boxes_orig):\n",
    "                iou = compute_iou(track_box_orig, det_box)\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_label = int(det_labels[i])\n",
    "            if best_iou < 0.3:\n",
    "                best_label = -1\n",
    "            if best_label >= 0 and best_label < len(class_names):\n",
    "                label_text = class_names[best_label]\n",
    "            else:\n",
    "                label_text = \"unknown\"\n",
    "\n",
    "            text = f\"ID:{track_id} {label_text}\"\n",
    "            cv2.rectangle(byte_frame, (x1_orig, y1_orig), (x2_orig, y2_orig), (0, 255, 0), 2)\n",
    "            cv2.putText(byte_frame, text, (x1_orig, y1_orig - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "        current_time = time.time()\n",
    "        fps = 1.0 / (current_time - prev_time)\n",
    "        prev_time = current_time\n",
    "        cv2.putText(byte_frame, f\"FPS: {fps:.2f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0), 2)\n",
    "\n",
    "        byte_out.write(byte_frame)\n",
    "        cv2.imshow(\"ByteTrack Output\", byte_frame)\n",
    "\n",
    "        # Update Open3D visualizer with LiDAR and Radar point clouds\n",
    "        if not lidar_added:\n",
    "            vis.add_geometry(lidar_pcd)\n",
    "            lidar_added = True\n",
    "        if not radar_added:\n",
    "            vis.add_geometry(radar_pcd)\n",
    "            radar_added = True\n",
    "        vis.update_geometry(lidar_pcd)\n",
    "        vis.update_geometry(radar_pcd)\n",
    "        vis.poll_events()\n",
    "        vis.update_renderer()\n",
    "\n",
    "        # Use a slightly longer waitKey to help the OS process GUI events\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        frame_count += 1\n",
    "        # Reduce sleep to give more time to GUI events (or remove if necessary)\n",
    "        time.sleep(0.001)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "        break\n",
    "\n",
    "overall_end_time = time.time()\n",
    "total_time = overall_end_time - overall_start_time\n",
    "avg_fps = frame_count / total_time\n",
    "print(f\"Average FPS: {avg_fps:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e5b266-3a41-4bc6-bbf1-136c35c09f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown\n",
    "cv2.destroyAllWindows()\n",
    "vis.destroy_window()\n",
    "byte_out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
